{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1b230f4-6c99-4674-bc18-8452a0adb0a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nマルコフ決定過程\\n行動によってstateが変わる\\n時間の概念が入る\\n報酬の総和を最大化することが求められる（目先の報酬ではない）\\n\\n状態遷移：状態はどのように遷移するか\\n報酬：報酬はどのように与えられるか\\n方策：エージェントはどのように行動を決定するか\\nを定式化\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "マルコフ決定過程\n",
    "行動によってstateが変わる\n",
    "時間の概念が入る\n",
    "報酬の総和を最大化することが求められる（目先の報酬ではない）\n",
    "\n",
    "状態遷移：状態はどのように遷移するか（状態遷移関数、状態遷移確率\n",
    "報酬：報酬はどのように与えられるか（報酬関数\n",
    "方策：エージェントはどのように行動を決定するか（\n",
    "を定式化\n",
    "\n",
    "MDP\n",
    "マルコフ性の仮定によって状態遷移（と報酬）をモデル化\n",
    "方策によって決められた行動と状態遷移確率によって、次の状態に遷移すると同時に報酬関数に従って報酬を得る\n",
    "最適方策（収益が最大となる方策）を見つけることがMDPの目標\n",
    "\n",
    "エピソードタスク：終わりのある問題（勝ち負け引き分け\n",
    "連続タスク：終わりがない（在庫管理\n",
    "\n",
    "収益を最大にすることがエージェントの目標\n",
    "収益は報酬の和、時間が進むに従い割引つによって報酬が減衰していく\n",
    "\n",
    "エージェントと環境は確率的に振る舞うかもしれない、確率的な挙動に対応するために収益（ある状態から行動した結果、つまり状態遷移）の期待値（＝状態価値関数）を指標とする\n",
    "状態によって状態価値関数の大小が変わる時は方策に優劣をつけられない\n",
    "最適な方策とは「どの表作と比較しても、全ての状態において状態価値関数の値が大きい方策」\n",
    "MDPでは最適方策（決定的）が少なくとも1つは存在することがわかっている\n",
    "最適状態価値関数（最適方策における状態価値関数）\n",
    "\n",
    "方策ごとに状態価値関数の値（期待値）は各状態で求める、そして各状態において他の方策よりも状態価値関数の値がおきいときそれが最適な方策\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4870d008-d365-4d15-8651-67461138be09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ベルマン方程式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffef5c4a-99e5-48db-af28-2b42a586cba3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nある状態s・時刻tにおける\\n報酬(R)の期待値 = ある状態(s)における、全ての行動の確率（＝方策）(π)*全ての状態の確率(p)*それぞれで得られる報酬(報酬関数r)\\n収益(G_t+1)の期待値 = ある状態(s)における、全ての行動の確率（＝方策）(π)*全ての状態の確率(p)*次の状態の収益(G_t+1)の期待値（=次の状態の状態価値関数）\\n\\nベルマン方程式によって無限に続く計算を有限の連立方程式に変換することができた\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "ある状態s・時刻tにおける\n",
    "報酬(R)の期待値 = ある状態(s)における、全ての行動の確率（＝方策）(π)*全ての状態の確率(p)*それぞれで得られる報酬(報酬関数r)\n",
    "収益(G_t+1)の期待値 = ある状態(s)における、全ての行動の確率（＝方策）(π)*全ての状態の確率(p)*次の状態の収益(G_t+1)の期待値（=次の状態の状態価値関数）\n",
    "\n",
    "ベルマン方程式によって無限に続く計算を有限の連立方程式に変換することができた\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d539f020-726f-4526-82e8-53b88090ae97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 行動価値関数（Q関数）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3cf5e8e0-2ef4-47b9-a699-db16ee346600",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n状態価値関数の条件は方策と状態\\n行動aも付け足したのが行動価値関数\\n行動は方策とは関係がない、行動をとった後に方策に従って行動する\\n状態価値関数は方策に従って行動を選ぶ\\n行動価値関数は自由に行動を選ぶ\\n\\nQ関数の行動aを方策πに従って選ぶとしたら\\n収益の期待値 = Q関数の重みつき和（方策π*q()の和)\\n\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "状態価値関数の条件は方策と状態\n",
    "行動aも付け足したのが行動価値関数\n",
    "行動は方策とは関係がない、行動をとった後に方策に従って行動する\n",
    "状態価値関数は方策に従って行動を選ぶ\n",
    "行動価値関数は自由に行動を選ぶ\n",
    "\n",
    "Q関数の行動aを方策πに従って選ぶとしたら\n",
    "収益の期待値 = Q関数の重みつき和（方策π*q()の和)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3dc369b-a8ea-410d-8c92-4a2d81c807b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ベルマン最適方程式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec424b6d-94b3-4b7d-a486-ba6aaa9204fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n方策が最適であるという性質を利用するとベルマン方程式をシンプルに表せる\\n状態の価値関数」と「その次に取り得る状態の価値関数」との関係性を表した式です。このベルマン方程式は、すべての状態とすべての方策について成り立ちます\\n\\n最適方策は、ある状態では必ずある特定の行動を選ぶ方策\\u3000＝\\u3000決定的\\u3000＝\\u3000関数であらわせる\\n最適行動価値関数（最適方策における行動価値関数）がわかっていれば、最適方策を得られる\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "方策が最適であるという性質を利用するとベルマン方程式をシンプルに表せる\n",
    "ベルマン方程式は状態の価値関数」と「その次に取り得る状態の価値関数」との関係性を表した式です。このベルマン方程式は、すべての状態とすべての方策について成り立ちます\n",
    "\n",
    "最適方策は、ある状態では必ずある特定の行動を選ぶ方策　＝　決定的　＝　関数であらわせる\n",
    "最適行動価値関数（最適方策における行動価値関数）がわかっていれば、最適方策を得られる\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fb39e6-0787-48b1-ab4f-f6bd2608b7af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
