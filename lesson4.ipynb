{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "352f00c1-b927-4151-ac78-948527ba573d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n（状態遷移確率、報酬関数、方策）-> ベルマン方程式 -> 連立方程式 -> 状態価値関数\\n連立方程sきを明示的に出して直接解くのは現実的には不可能\\nよって動的計画法（DP)\\n\\n強化学習の２つのタスク\\n方策評価\\u3000方策が与えられた時に価値関数を求めること\\n方策制御\\u3000方策を最適方策へ調整すること\\n\\n評価-> 制御\\u3000の流れ、DPで方策評価を行う\\n\\n反復方策評価（DPの具体的なアルゴリズム）\\nベルマン方程式を更新式（ブートストラッピング、推定値を使って推定値を改善する）に変えたもの\\n\\n状態遷移が決定的：ある状態sである行動aを行えば次の状態s'は１つに決まる\\n\\n\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "（状態遷移確率、報酬関数、方策）-> ベルマン方程式 -> 連立方程式 -> 状態価値関数\n",
    "連立方程sきを明示的に出して直接解くのは現実的には不可能\n",
    "よって動的計画法（DP)\n",
    "\n",
    "強化学習の２つのタスク\n",
    "方策評価　方策が与えられた時に価値関数を求めること\n",
    "方策制御　方策を最適方策へ調整すること\n",
    "\n",
    "評価-> 制御　の流れ、DPで方策評価を行う\n",
    "\n",
    "反復方策評価（DPの具体的なアルゴリズム）\n",
    "ベルマン方程式を更新式（ブートストラッピング、推定値を使って推定値を改善する）に変えたもの\n",
    "\n",
    "状態遷移が決定的：ある状態sである行動aを行えば次の状態s'は１つに決まる\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd3336ee-beea-40d5-96bb-e3c2fcc54193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 反復方策評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "def5ae8d-1700-4ba5-8066-fff351ab2cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73ffdcf1-a3d0-49a8-be2c-bbf094a6b02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld:\n",
    "    def __init__(self):\n",
    "        '''\n",
    "                          | goal\n",
    "                          v\n",
    "        +---+------+---+------+\n",
    "        | 0 |  0   | 0 |  1.0 |\n",
    "        +---+------+---+------+\n",
    "        | 0 | None | 0 | -1.0 |   \n",
    "        +---+------+---+------+\n",
    "        | 0 |  0   | 0 |   0  |\n",
    "        +---+------+---+------+\n",
    "          ^\n",
    "          | start\n",
    "        '''\n",
    "        self.action_space = [0,1,2,3] \n",
    "        self.action_meaning = {\n",
    "            0: \"UP\",\n",
    "            1: \"DOWN\",\n",
    "            2: \"LEFT\",\n",
    "            3: \"RIGHT\",\n",
    "        }\n",
    "        self.reward_map = np.array(\n",
    "            [\n",
    "                [0,0,0,1.0],\n",
    "                [0,None,0,-1.0],\n",
    "                [0,0,0,0]\n",
    "            ]\n",
    "        )\n",
    "        self.goal_state = (0, 3)\n",
    "        self.wall_state = (1, 1)\n",
    "        self.start_state = (2, 0)\n",
    "        self.agent_state = self.start_state\n",
    "\n",
    "    @property\n",
    "    def height(self):\n",
    "        return len(self.reward_map)\n",
    "\n",
    "    @property\n",
    "    def width(self):\n",
    "        return len(self.reward_map[0])\n",
    "\n",
    "    @property\n",
    "    def shape(self):\n",
    "        return len(self.reward_map.shape)\n",
    "\n",
    "    def actions(self):\n",
    "        return self.action_space\n",
    "\n",
    "    def states(self):\n",
    "        for h in range(self.height):\n",
    "            for w in range(self.width):\n",
    "                yield (h, w)\n",
    "\n",
    "    def next_state(self, state, action):\n",
    "        action_move_map = [(-1,0),(1,0),(0,1),(0,-1)]\n",
    "        move = action_move_map[action]\n",
    "        next_state = (state[0]+move[0], state[1]+move[1])\n",
    "        ny, nx = next_state\n",
    "\n",
    "        if nx<0 or nx>=self.width or ny<0 or ny>=self.height:\n",
    "            next_state = state\n",
    "        elif next_state==self.wall_state:\n",
    "            next_state = state\n",
    "\n",
    "        return next_state\n",
    "\n",
    "    def reward(self, state, action, next_state):\n",
    "        return self.reward_map[next_state]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a33a96a1-edd6-4bdf-afc1-b1e03616bb24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(0, 0): 1.2159087003296114, (0, 1): -0.5806169180349051, (0, 2): -1.0381794008048761, (0, 3): 0.8253526350196313, (1, 0): 0.2067356038063959, (1, 1): -0.7160572913410463, (1, 2): -0.6053089184072472, (1, 3): -0.5527016421412936, (2, 0): -0.40616378455141067, (2, 1): -0.8156943789802026, (2, 2): -0.7535806204959032, (2, 3): 0.6389817441476344}\n"
     ]
    }
   ],
   "source": [
    "env = GridWorld()\n",
    "V = {}\n",
    "for state in env.states():\n",
    "    V[state] = np.random.randn() # 状態価値関数\n",
    "\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c7188bf-6fe3-4803-a641-516475ef6aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ddb3c62d-f774-412f-8b08-bbecd7288270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.25, 1: 0.25, 2: 0.25, 3: 0.25}\n"
     ]
    }
   ],
   "source": [
    "# ランダムな方策\n",
    "pi = defaultdict(lambda: {\n",
    "    0: 0.25,\n",
    "    1: 0.25,\n",
    "    2: 0.25,\n",
    "    3: 0.25\n",
    "})\n",
    "state = (0, 1)\n",
    "print(pi[state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "77a3198b-e90c-4e48-ad80-ac7ddaf76ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_onestep(pi, V, env, gamma=0.9):\n",
    "    for state in env.states():  # ①各状態へアクセス\n",
    "        if state == env.goal_state:  # ②ゴールの価値関数は常に0\n",
    "            V[state] = 0\n",
    "            continue\n",
    "\n",
    "        action_probs = pi[state]  \n",
    "        new_V = 0\n",
    "\n",
    "        # ③各行動へアクセス\n",
    "        for action, action_prob in action_probs.items():\n",
    "            next_state = env.next_state(state, action)\n",
    "            r = env.reward(state, action, next_state)\n",
    "            # ④新しい価値関数\n",
    "            new_V += action_prob * (r + gamma * V[next_state])\n",
    "        V[state] = new_V\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1921d3d4-caf2-4933-ba2f-b3ee4e8038fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_eval(pi, V, env, gamma, threshold=0.001):\n",
    "    while True:\n",
    "        old_V = V.copy()  # 更新前の価値関数\n",
    "        V = eval_onestep(pi, V, env, gamma)\n",
    "\n",
    "        # 更新された量の最大値を求める\n",
    "        delta = 0\n",
    "        for state in V.keys():\n",
    "            t = abs(V[state] - old_V[state])\n",
    "            if delta < t:\n",
    "                delta = t\n",
    "\n",
    "        # 閾値との比較\n",
    "        if delta < threshold:\n",
    "            break\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "62ad896a-32d5-4699-a9ab-9f317ca37280",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridWorld()\n",
    "gamma = 0.9\n",
    "\n",
    "pi = defaultdict(lambda: {0: 0.25, 1: 0.25, 2: 0.25, 3: 0.25})\n",
    "V = defaultdict(lambda: 0)\n",
    "\n",
    "V = policy_eval(pi, V, env, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8982e2c1-43df-4f16-8440-9a0d6ac5150c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<function <lambda> at 0x10709dbc0>, {(0, 0): 0.03028806099856353, (1, 0): -0.026936362355998215, (0, 1): 0.0977824241089171, (0, 2): 0.2071980803453905, (1, 2): -0.495978151235659, (0, 3): 0, (2, 0): -0.09886155432366785, (2, 1): -0.21729033457922944, (1, 1): -0.14427715375541272, (2, 2): -0.43435951415465596, (1, 3): -0.37134421613448265, (2, 3): -0.7838088155423875})\n"
     ]
    }
   ],
   "source": [
    "print(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4d6f9c2f-95f0-4092-a8c6-75f80daa67d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 方策を改善する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "61022890-16a0-43ee-a7cc-2670004203d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n最適方策は最大値を取る行動aによって決まる\\n\\n最適方策ではなく「なんらかの決定論的方策」に対しても最大値を取る行動aによって方策を更新する（＝greedy化と呼ぶことにする）と\\n常に改善されるか、改善が止まればそれが最適方策になる\\n\\nこうして得た方策（その時は最適）を評価して価値関数を得る、その価値関数でgreedy化をして方策を得る（この時点では最適）\\nこれを方策が変更されなくなるまで繰り返す（方策反復法）\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "最適方策は最大値を取る行動aによって決まる\n",
    "\n",
    "最適方策ではなく「なんらかの決定論的方策」に対しても最大値を取る行動aによって方策を更新する（＝greedy化と呼ぶことにする）と\n",
    "常に改善されるか、改善が止まればそれが最適方策になる\n",
    "\n",
    "こうして得た方策（その時は最適）を評価して価値関数を得る、その価値関数でgreedy化をして方策を得る（この時点では最適）\n",
    "これを方策が変更されなくなるまで繰り返す（方策反復法）\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aade8222-bafc-49b1-9783-2665d594bdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def argmax(d):\n",
    "    max_value = max(d.values())\n",
    "    max_key = 0\n",
    "    for key, value in d.items():\n",
    "        if value == max_value:\n",
    "            max_key = key\n",
    "    return max_key\n",
    "    \n",
    "def greedy_policy(V, env, gamma):\n",
    "    pi = {}\n",
    "\n",
    "    for state in env.states():\n",
    "        action_values = {}\n",
    "\n",
    "        for action in env.actions():\n",
    "            next_state = env.next_state(state, action)\n",
    "            r = env.reward(state, action, next_state)\n",
    "            value = r + gamma * V[next_state]  # ①\n",
    "            action_values[action] = value\n",
    "            max_action = argmax(action_values)  # ②\n",
    "            action_probs = {0: 0, 1: 0, 2: 0, 3: 0}\n",
    "            action_probs[max_action] = 1.0\n",
    "            pi[state] = action_probs  # ③\n",
    "    return pi\n",
    "\n",
    "def policy_iter(env, gamma, threshold=0.001, is_render=False):\n",
    "    pi = defaultdict(lambda: {0: 0.25, 1: 0.25, 2: 0.25, 3: 0.25})\n",
    "    V = defaultdict(lambda: 0)\n",
    "\n",
    "    while True:\n",
    "        V = policy_eval(pi, V, env, gamma, threshold)  # ①評価\n",
    "        new_pi = greedy_policy(V, env, gamma)  # ②改善\n",
    "\n",
    "        if is_render:\n",
    "            env.render_v(V, pi)\n",
    "\n",
    "        if new_pi == pi:  # ③更新チェック\n",
    "            break\n",
    "        pi = new_pi\n",
    "\n",
    "    return pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "76b771a7-0f46-4faa-bb61-9864d50da1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridWorld()\n",
    "gamma = 0.9\n",
    "pi = policy_iter(env, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cc3b829b-6604-4259-a9c7-8ca3407e0ac1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n方策反復法のアイデアは、図4-17のように「評価」と「改善」という2つの過程を交互に繰り返すこと\\n\\n方策評価と方策改善の2つの過程を交互に繰り返すアルゴリズムでは、評価と改善の“粒度”——どれぐらい正確に評価（もしくは改善）するかという点——について自由度があります。\\n「評価」フェーズでは完全に更新されなくても、真の価値関数の方向に近づきさえすれば良いということです。同様に「改善」フェーズではgreedy化の方向へ向かう（一部だけがgreedy化される）だけで良い\\n\\n価値反復法：評価と改善をそれぞれ最小限に行う、方策を使わずに、価値関数を更新\\n\\n方策反復法：すべての状態における価値関数を何度も更新します。その更新作業が収束したら、続いて「改善」（greedy化）のフェーズに移ります。\\n価値反復法：1つの状態だけを一度更新したら、すぐに「改善」フェーズへと移行する\\n\\n\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "方策反復法のアイデアは、図4-17のように「評価」と「改善」という2つの過程を交互に繰り返すこと\n",
    "\n",
    "方策評価と方策改善の2つの過程を交互に繰り返すアルゴリズムでは、評価と改善の“粒度”——どれぐらい正確に評価（もしくは改善）するかという点——について自由度があります。\n",
    "「評価」フェーズでは完全に更新されなくても、真の価値関数の方向に近づきさえすれば良いということです。同様に「改善」フェーズではgreedy化の方向へ向かう（一部だけがgreedy化される）だけで良い\n",
    "\n",
    "価値反復法：評価と改善をそれぞれ最小限に行う、方策を使わずに、価値関数を更新\n",
    "\n",
    "方策反復法：すべての状態における価値関数を何度も更新します。その更新作業が収束したら、続いて「改善」（greedy化）のフェーズに移ります。\n",
    "価値反復法：1つの状態だけを一度更新したら、すぐに「改善」フェーズへと移行する\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3e289137-eafa-4efb-9ab9-01dde5efb405",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iter_onestep(V, env, gamma):\n",
    "    for state in env.states():  # ①すべての状態にアクセス\n",
    "        if state == env.goal_state:  # ゴールの価値関数は常に0\n",
    "            V[state] = 0\n",
    "            continue\n",
    "\n",
    "        action_values = []\n",
    "        for action in env.actions():  # ②すべての行動にアクセス\n",
    "            next_state = env.next_state(state, action)\n",
    "            r = env.reward(state, action, next_state)\n",
    "            value = r + gamma * V[next_state]  # ③新しい価値観数\n",
    "            action_values.append(value)\n",
    "\n",
    "        V[state] = max(action_values)  # ④最大値を取り出す\n",
    "    return V\n",
    "\n",
    "def value_iter(V, env, gamma, threshold=0.001):\n",
    "    while True:\n",
    "        old_V = V.copy()  # 更新前の価値関数\n",
    "        V = value_iter_onestep(V, env, gamma)\n",
    "\n",
    "        # 更新された量の最大値を求める\n",
    "        delta = 0\n",
    "        for state in V.keys():\n",
    "            t = abs(V[state] - old_V[state])\n",
    "            if delta < t:\n",
    "                delta = t\n",
    "        # 閾値との比較\n",
    "        if delta < threshold:\n",
    "            break\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8fe9b840-3200-4def-80c2-8b7c285a81be",
   "metadata": {},
   "outputs": [],
   "source": [
    "V = defaultdict(lambda: 0)\n",
    "env = GridWorld()\n",
    "gamma = 0.9\n",
    "\n",
    "V = value_iter(V, env, gamma)\n",
    "\n",
    "pi = greedy_policy(V, env, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2b495655-53ae-4431-bb3f-cc48bd28c66a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<function <lambda> at 0x10709e160>, {(0, 0): 0.81, (1, 0): 0.7290000000000001, (0, 1): 0.9, (0, 2): 1.0, (1, 2): 0.9, (0, 3): 0, (2, 0): 0.6561000000000001, (2, 1): 0.7290000000000001, (1, 1): 0.81, (2, 2): 0.81, (1, 3): 1.0, (2, 3): 0.7290000000000001})\n"
     ]
    }
   ],
   "source": [
    "print(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b72a153-2fe6-45f7-8071-889cfb2f4c62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
